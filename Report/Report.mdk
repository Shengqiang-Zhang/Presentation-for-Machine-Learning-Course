Title         : 机器学习报告
Author        : 
Logo          : False
Package: [UTF8]ctex

[TITLE]

# 摘要

本次机器学习课程作业我们小组是选择参与了`Kaggle`竞赛平台上当前一道比较热门的竞赛题：“Sberbank Russian Housing Market------
Can you predict realty price fluctuations in Russia’s volatile economy?”，主要内容是让我们根据提供的俄罗斯房价数据和俄罗斯经济情况数据来建立机器学习模型，用来预测房价。我们小组所做的工作主要有对提供的数据进行分析和处理，然后用`xgboost`工具对数据进行训练，经过调参，最终得到一个比较满意的房价预测模型，我们的结果提交后在2000多支参赛队伍中最高排名为200多名。

# 小组成员及分工
## 小组成员信息
* 邱雷   53140306  (组长)
* 张盛强 53140320
* 代明玉 53140305
* 加永郎加  53140308

## 成员分工

* 邱雷：数据处理部分
* 张盛强：数据分析部分
* 代明玉：训练模型
* 加永郎加：训练模型

# 题目描述
Housing costs demand a significant investment from both consumers and developers. And when it comes to planning a budget—whether personal or corporate—the last thing anyone needs is uncertainty about one of their biggets expenses. Sberbank, Russia’s oldest and largest bank, helps their customers by making predictions about realty prices so renters, developers, and lenders are more confident when they sign a lease or purchase a building.

Although the housing market is relatively stable in Russia, the country’s volatile economy makes forecasting prices as a function of apartment characteristics a unique challenge. Complex interactions between housing features such as number of bedrooms and location are enough to make pricing predictions complicated. Adding an unstable economy to the mix means Sberbank and their customers need more than simple regression models in their arsenal.

In this competition, Sberbank is challenging Kagglers to develop algorithms which use a broad spectrum of features to predict realty prices. Competitors will rely on a rich dataset that includes housing data and macroeconomic patterns. An accurate forecasting model will allow Sberbank to provide more certainty to their customers in an uncertain economy.

# 数据分析
## 题目提供的数据集
题目提供了一下4个数据集：

* data_dictionary.txt
* macro.csv
* train.csv
* test.csv

其中，

* `data_dictionary.txt`是数据字典，包含了对`train.csv`，`macro.csv`，`test.csv` 数据集中数据项的定义，以及对相近特征的细致描述。
* `macro.csv`是描述俄罗斯经济情况的数据集
* `train.csv`是训练集，包含了很多特征数据以及对应的房价数据
* `test.csv`是测试集

下面对这几个数据集做具体说明。

## 数据集`macro.csv`
我们首先输出macro.csv数据集的大小和特征：
```Python
  print(macro.shape)
  print(macro.columns.values)
 ```
部分结果显示如下：

![捕获]
[捕获]: images/-.PNG "捕获" { width:auto; max-width:100% }
从中我们可知该数据的大小为$ 2404 \times 100$，特征项为100个。

我们再将`macro.csv`的前5项输出：
```Python
  print(macro.head())
  ```
部分结果显示如下：

![捕获2]

[捕获2]: images/-2.PNG "捕获2" { width:auto; max-width:100% }

从中我们可以看出，很多数据为空。

## 数据集`train.csv`
我们首先输出`train.csv`数据集的大小和特征：
```Python
  print(train.shape)
  print(train.columns.values)
```
部分结果显示如下：

![捕获3]

[捕获3]: images/-3.PNG "捕获3" { width:500pt; max-width:100% }

从中我们可以看出数据集的大小为$30471\times292$，特征项为292个。

我们再将`train.csv`的前5项输出：
```Python
  print(train.head())
  ```
部分结果显示如下：

![捕获4]

[捕获4]: images/-4.PNG "捕获4" { width:auto; max-width:100% }

再来看看数据的分布情况：
```Python
  missingValueColumns = train.columns[train.isnull().any()].tolist()
  msno.bar(train[missingValueColumns],figsize=(20,8),color=(0.5, 0.5, 1),fontsize=12,\
          labels=True)
  msno.matrix(train[missingValueColumns],width_ratios=(10,1),figsize=(20,8),color=\
            (0.5, 0.5, 1),fontsize=12,sparkline=True,labels=True)
  ```
结果显示如下：

![1]

[1]: images/1.png "1" { width:auto; max-width:100% }

![QQ图片20170613090738]

[QQ图片20170613090738]: images/QQ-20170613090738.png "QQ图片20170613090738" { width:auto; max-width:100% }

# 数据处理
我们处理数据主要是去除一些错误的数据和增加一些有利的特征。
## 去除错误的数据
* 房屋面积错误

  数据集中多处房屋面积信息存在错误，我们将这些数据项都设置为空项。
  ```Python
    bad_index = train[train.life_sq > train.full_sq].index
    train.ix[bad_index, "life_sq"] = np.NaN
    equal_index = [601, 1896, 2791]
    test.ix[equal_index, "life_sq"] = test.ix[equal_index, "full_sq"]
    bad_index = test[test.life_sq > test.full_sq].index
    test.ix[bad_index, "life_sq"] = np.NaN
    bad_index = train[train.life_sq < 5].index
    train.ix[bad_index, "life_sq"] = np.NaN
    bad_index = test[test.life_sq < 5].index
    test.ix[bad_index, "life_sq"] = np.NaN
    bad_index = train[train.full_sq < 5].index
    train.ix[bad_index, "full_sq"] = np.NaN
    bad_index = test[test.full_sq < 5].index
    test.ix[bad_index, "full_sq"] = np.NaN
    ```
  ```Python
    bad_index = train[train.kitch_sq >= train.life_sq].index
    train.ix[bad_index, "kitch_sq"] = np.NaN
    bad_index = test[test.kitch_sq >= test.life_sq].index
    test.ix[bad_index, "kitch_sq"] = np.NaN
    bad_index = train[(train.kitch_sq == 0).values + (train.kitch_sq == 1).values].index
    train.ix[bad_index, "kitch_sq"] = np.NaN
    bad_index = test[(test.kitch_sq == 0).values + (test.kitch_sq == 1).values].index
    test.ix[bad_index, "kitch_sq"] = np.NaN
    bad_index = train[(train.full_sq > 210) & (train.life_sq / train.full_sq < 0.3)].index
    train.ix[bad_index, "full_sq"] = np.NaN
    bad_index = test[(test.full_sq > 150) & (test.life_sq / test.full_sq < 0.3)].index
    test.ix[bad_index, "full_sq"] = np.NaN
    bad_index = train[train.life_sq > 300].index
    train.ix[bad_index, ["life_sq", "full_sq"]] = np.NaN
    bad_index = test[test.life_sq > 200].index
    test.ix[bad_index, ["life_sq", "full_sq"]] = np.NaN
    ```

* 房屋建设年代错误

     数据集中有些房屋建设年代存在错误，例如建设年代过早，我们将建设年代早于1500年的数据项都置为空。
```Python
    bad_index = train[train.build_year < 1500].index
    train.ix[bad_index, "build_year"] = np.NaN
    bad_index = test[test.build_year < 1500].index
    test.ix[bad_index, "build_year"] = np.NaN
  ```
* 房间数量错误
```Python
    bad_index = train[train.num_room == 0].index
    train.ix[bad_index, "num_room"] = np.NaN
    bad_index = test[test.num_room == 0].index
    test.ix[bad_index, "num_room"] = np.NaN
    bad_index = [10076, 11621, 17764, 19390, 24007, 26713, 29172]
    train.ix[bad_index, "num_room"] = np.NaN
    bad_index = [3174, 7313]
    test.ix[bad_index, "num_room"] = np.NaN
  ```
* 楼层信息错误

```Python
        bad_index = train[(train.floor == 0).values * (train.max_floor == 0).values].index
        train.ix[bad_index, ["max_floor", "floor"]] = np.NaN
        bad_index = train[train.floor == 0].index
        train.ix[bad_index, "floor"] = np.NaN
        bad_index = train[train.max_floor == 0].index
        train.ix[bad_index, "max_floor"] = np.NaN
        bad_index = test[test.max_floor == 0].index
        test.ix[bad_index, "max_floor"] = np.NaN
        bad_index = train[train.floor > train.max_floor].index
        train.ix[bad_index, "max_floor"] = np.NaN
        bad_index = test[test.floor > test.max_floor].index
        test.ix[bad_index, "max_floor"] = np.NaN
    ```
    
## 增加有利的特征
* 增加`month-year`特征
```Python
  month_year = (train.timestamp.dt.month + train.timestamp.dt.year * 100)
  month_year_cnt_map = month_year.value_counts().to_dict()
  train['month_year_cnt'] = month_year.map(month_year_cnt_map)

  month_year = (test.timestamp.dt.month + test.timestamp.dt.year * 100)
  month_year_cnt_map = month_year.value_counts().to_dict()
  test['month_year_cnt'] = month_year.map(month_year_cnt_map)
  ```
* 增加`week-year count`特征
```Python
  week_year = (train.timestamp.dt.weekofyear + train.timestamp.dt.year * 100)
  week_year_cnt_map = week_year.value_counts().to_dict()
  train['week_year_cnt'] = week_year.map(week_year_cnt_map)

  week_year = (test.timestamp.dt.weekofyear + test.timestamp.dt.year * 100)
  week_year_cnt_map = week_year.value_counts().to_dict()
  test['week_year_cnt'] = week_year.map(week_year_cnt_map)
  ```
* 增加`month and day-of-week`特征
```Python
  train['month'] = train.timestamp.dt.month
  train['dow'] = train.timestamp.dt.dayofweek

  test['month'] = test.timestamp.dt.month
  test['dow'] = test.timestamp.dt.dayofweek
  ```
* 其它特征工程
```Python
  train['rel_floor'] = train['floor'] / train['max_floor'].astype(float)
  train['rel_kitch_sq'] = train['kitch_sq'] / train['full_sq'].astype(float)

  test['rel_floor'] = test['floor'] / test['max_floor'].astype(float)
  test['rel_kitch_sq'] = test['kitch_sq'] / test['full_sq'].astype(float)

  train.apartment_name = train.sub_area + train['metro_km_avto'].astype(str)
  test.apartment_name = test.sub_area + train['metro_km_avto'].astype(str)

  train['room_size'] = train['life_sq'] / train['num_room'].astype(float)
  test['room_size'] = test['life_sq'] / test['num_room'].astype(float)
  ```

# 模型训练
## 选择`xgboost`的原因
* `xgboost`是大规模并行`boosted tree`的工具，它是目前最快最好的开源`boosted tree`工具包

* 由于其高效的`C++`实现，`xgboost`在性能上超过了最常用使用的R包`gbm`和`Python`包`sklearn`。

* 在数据科学方面，有大量`kaggle`选手选用它进行数据挖掘比赛，其中包括两个以上`kaggle`比赛的夺冠方案。

* 自带处理空数据

## `xgboost`参数设置及训练过程
我们在经过调参以后确定了以下`xgboost`参数，并将训练集`train.csv`和`macro.csv`运用于训练。
```Python
  def xg(x_train, y_train, x_test):
    xgb_params = {
        'eta': 0.07,
        'max_depth': 5,
        'min_child_weight': 5,
        'gamma': 0,
        'max_delta_step': 0,
        'subsample': 0.8,
        'colsample_bytree': 0.7,
        'objective': 'reg:linear',
        'eval_metric': 'rmse',
        'silent': 1
    }

    dtrain = xgb.DMatrix(x_train, y_train)
    dtest = xgb.DMatrix(x_test)
    model = xgb.train(dict(xgb_params, silent=0), dtrain, num_boost_round=350)
    y_predict = model.predict(dtest)
    y_predict = np.round(y_predict)
    return y_predict
    ```
# 结果
在xgboost训练完成后，将测试集`test.csv`输入模型得到预测结果。我们在结果提交后，排名在3000个参赛队伍中最高为200名左右，目前的排名为`513/2922`

